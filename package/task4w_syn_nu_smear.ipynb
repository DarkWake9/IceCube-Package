{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedure for smearing\n",
    "---------------------------------\n",
    "There are two things you have to account for : muon-> neutrino scattering angle and error in reconstruction of muon direction.\n",
    "\n",
    "The first  quantity is given in the variables PSF_min[deg]  PSF_max[deg] and the second in AngErr_min[deg]  AngErr_max[deg] in the files\n",
    "\n",
    "1. For each neutrino energy and declination choose the average of PSF_min and PSF_max as well as average of AngErr_min[deg]  AngErr_max[deg]  and add them in quadrature. \n",
    "        This will give you the total angular uncertainty. ($\\theta_{tot}$)\n",
    "\n",
    "- For each simulated neutrino, take average of PSF_min and PSF_max as well as average of AngErr_min[deg]  AngErr_max[deg] and calculate $\\sqrt{PSF_{avg}^2 + AngErr_{avg}^2}$\n",
    "\n",
    "2. convert the simulated RA and DEC to theta  ($\\theta_{mock}$) and phi  ($\\phi_{mock}$) for the simulated MJD \n",
    "\n",
    "- Choose simulated MJD  to be uniformly distributed for each phase\n",
    "\n",
    "3. assume error in theta=error in phi =($\\theta_{tot}$)/$\\sqrt(2)$\n",
    "\n",
    "then choose Gausian distributed random numbers with **mean=0** and standard deviation = error in theta as well as error in phi.\n",
    "\n",
    "This will give you the error in theta  ($\\Delta \\theta_{mock}$) and error in phi  ($\\Delta \\phi_{mock}$) for the simulated neutrino event\n",
    "\n",
    "New theta=$\\theta_{mock}$+$\\Delta \\theta_{mock}$\n",
    "\n",
    "and New phi=$\\phi_{mock}$+$\\Delta \\phi_{mock}$\n",
    "\n",
    "4. Calculate new RA and DEC from new theta and new phi . This will be the smeared RA and DEC of the muon\n",
    "\n",
    "5. as a sanity check calculate the angle between the smeared muon and the original pulsar direction. It should be close to $\\theta_{tot}$.\n",
    "\n",
    "Again all this should be in a separate code and decoupled from the routine to \n",
    "do the search.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.units import deg, meter\n",
    "from astropy.coordinates import ICRS, get_sun, SkyCoord, AltAz, EarthLocation\n",
    "from astropy.time import Time\n",
    "####syn_nu_ze\n",
    "import numpy as np\n",
    "import os\n",
    "import multiprocessing as mul\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import jit, njit, prange, set_num_threads, vectorize, guvectorize, cuda\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read icdata\n",
      "read uptdata\n",
      "read eadata\n",
      "read mspdata\n",
      "[36900, 107011, 93133, 136244, 112855, 122539, 127040, 129307, 123654, 145748]\n"
     ]
    }
   ],
   "source": [
    "os.system(\"cp -r ./o_data/icecube_10year_ps/events/ ./data/icecube_10year_ps/events/\")\n",
    "\n",
    "from core.signal_bag import *\n",
    "from core.stacking_analysis import *\n",
    "from core.req_arrays import *\n",
    "print(icwidths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude = -89.99\n",
    "longitude = 13.2536#-63.453056\n",
    "location = EarthLocation(lat=latitude * deg, lon=longitude * deg, height = 2835*meter)\n",
    "\n",
    "def ra_dec_to_azimuth_zenith(ra_deg, dec_deg, obs_mjd):\n",
    "    \n",
    "    # Convert MJD to Time object\n",
    "    obs_time = Time(obs_mjd, format='mjd')\n",
    "\n",
    "    # Create a SkyCoord object with the given RA and Dec\n",
    "    coord = SkyCoord(ra=ra_deg * deg, dec=dec_deg* deg, frame='icrs')\n",
    "\n",
    "    # Convert to Altitude and Azimuth coordinates\n",
    "    altaz = coord.transform_to(AltAz(location=location ,obstime=obs_time))\n",
    "\n",
    "    # Calculate zenith angle (90° - altitude)\n",
    "    # zenith_angle_deg = 90.0 - altaz.alt.deg\n",
    "    alt_angle_deg = altaz.alt.deg\n",
    "\n",
    "    # Extract Azimuth and Zenith Angle values (in degrees)\n",
    "    azimuth_deg = altaz.az.deg\n",
    "    \n",
    "    return (azimuth_deg, alt_angle_deg)\n",
    "\n",
    "\n",
    "def altaz_to_radec(azimuth, alt, mjd):\n",
    "    \n",
    "    # Convert zenith angle to altitude\n",
    "    # altitude = (90 - zenith) * deg\n",
    "    altitude = alt * deg\n",
    "    \n",
    "    # Convert MJD to Time object\n",
    "    observing_time = Time(mjd, format='mjd', scale='utc')\n",
    "    \n",
    "    # Create EarthLocation\n",
    "    \n",
    "    # Create AltAz coordinate\n",
    "    altaz = AltAz(location=location, obstime=observing_time)\n",
    "    \n",
    "    # Create SkyCoord from AltAz\n",
    "    altaz_coord = SkyCoord(alt=altitude, az=azimuth, frame=altaz, unit=(deg, deg))\n",
    "    \n",
    "    # Convert AltAz coordinates to RA and Dec\n",
    "    equatorial_coord = altaz_coord.transform_to('icrs')\n",
    "    \n",
    "    \n",
    "    ra_deg = equatorial_coord.ra.deg\n",
    "    dec_deg = equatorial_coord.dec.deg\n",
    "    \n",
    "    return (ra_deg, dec_deg)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "icsmear_k = pd.read_csv('./data/icecube_10year_ps/irfs/IC40_smearing.csv', sep='\\s+', comment='#', names='log10(E_nu/GeV)_min\tlog10(E_nu/GeV)_max\tDec_nu_min[deg]\tDec_nu_max[deg]\tlog10(E/GeV)_min\tlog10(E/GeV)_max\tPSF_min[deg]\tPSF_max[deg]\tAngErr_min[deg]\tAngErr_max[deg]\tFractional_Counts'.split('\\t'), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "icsmear_k_log_E = np.array(list(set(icsmear_k['log10(E_nu/GeV)_min'])))#.union(set(icsmear_k['log10(E_nu/GeV)_max']))))\n",
    "log_E_width = 26400\n",
    "icsmear_k_log_E.sort()\n",
    "\n",
    "# icsmear_k_log_E = 10**(icsmear_k_log_E+9)\n",
    "\n",
    "# icsmear_k_dec = np.array(list(set(icsmear_k['Dec_nu_min[deg]']).union(set(icsmear_k['Dec_nu_max[deg]']))))\n",
    "icsmear_k_dec = np.ravel(list(set(icsmear_k['Dec_nu_min[deg]'])))\n",
    "icsmear_k_psf = np.array(list(set(icsmear_k['PSF_min[deg]']).union(set(icsmear_k['PSF_max[deg]']))))\n",
    "\n",
    "icsmear_k_dec.sort()\n",
    "dec_width = 123200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 12292.80it/s]\n",
      "100%|██████████| 100/100 [00:44<00:00,  2.27it/s]\n",
      " 20%|██        | 20/100 [00:43<04:33,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with season 0\n",
      "0 42556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:10<00:00,  1.43it/s]\n",
      " 45%|████▌     | 45/100 [01:10<00:28,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with season 1\n",
      "1 117430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:14<00:00,  1.33it/s]\n",
      " 41%|████      | 41/100 [01:15<00:47,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with season 2\n",
      "2 106187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:22<00:00,  1.22it/s]\n",
      " 38%|███▊      | 38/100 [01:22<01:14,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with season 8\n",
      "8 138518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:31<00:00,  1.10it/s]\n",
      "100%|██████████| 100/100 [01:32<00:00,  1.09it/s]\n",
      " 80%|████████  | 80/100 [01:32<00:01, 14.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with season 3\n",
      "3 152482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 63/100 [01:32<00:03, 12.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with season 7\n",
      "7 145481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:33<00:00,  1.07it/s]\n",
      "100%|██████████| 100/100 [01:33<00:00,  1.07it/s]\n",
      " 81%|████████  | 81/100 [01:33<00:01, 17.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with season 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 97/100 [01:33<00:00, 16.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 128099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:33<00:00,  1.07it/s]\n",
      " 87%|████████▋ | 87/100 [01:33<00:00, 17.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with season 6\n",
      "6 143625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 95/100 [01:34<00:00, 18.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with season 5\n",
      "5 138303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:34<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with season 9\n",
      "9 162956\n"
     ]
    }
   ],
   "source": [
    "season = 0\n",
    "nbins=100\n",
    "# if arrg.numbins:\n",
    "#     nbins = arrg.numbins\n",
    "# else:\n",
    "#     nbins = 100\n",
    "\n",
    "n_psrs = 30\n",
    "# if arrg.numpulsars:\n",
    "#     n_psrs = arrg.numpulsars\n",
    "\n",
    "enus = np.logspace(11.001, 18.999, int(nbins))\n",
    "# enus_bin_indices = np.zeros(len(enus), dtype=np.int64)\n",
    "\n",
    "# for i in prange(len(enus)):\n",
    "enus_bin_indices = np.digitize(enus, e_nu_wall) - 1\n",
    "\n",
    "\n",
    "# msdec_bin_indices = np.zeros(p, dtype=np.int64)\n",
    "# for i in prange(p):\n",
    "msdec_bin_indices = np.digitize(msdec, dec_nu) - 1\n",
    "\n",
    "\n",
    "gamma_arr = [-2, -2.2, -2.53, -3]\n",
    "# phio = np.logspace(-38, -26, 1000) #CHANGING TO LINEAR BINS RESULTS IN STRAIGHT LINES\n",
    "\n",
    "####\n",
    "syn_nu_choice = np.random.randint(0, p, n_psrs) #Choose 50 random pulsars from the 3389 pulsars\n",
    "syn_nudec_bin = msdec_bin_indices[syn_nu_choice] #Find the declination bin of the chosen pulsars to be allocated for the synthetic neutrinos\n",
    "syn_nu_ra = msra[syn_nu_choice] #Find the right ascension of the chosen pulsars to be allocated for the synthetic neutrinos\n",
    "syn_nu_dec = msdec[syn_nu_choice] #Find the declination of the chosen pulsars to be allocated for the synthetic neutrinos\n",
    "phio_const = 4.98 * (10**(-27)) #GeV-1 to ev-1 conversion factor\n",
    "# phio_const *= 1e-3\n",
    "filenames = [\"IC40_exp.csv\", \"IC59_exp.csv\",\"IC79_exp.csv\", \"IC86_I_exp.csv\", \"IC86_II_exp.csv\",\n",
    "        \"IC86_III_exp.csv\", \"IC86_IV_exp.csv\", \"IC86_V_exp.csv\", \"IC86_VI_exp.csv\", \"IC86_VII_exp.csv\"]\n",
    "\n",
    "# syn_nu_dec = []\n",
    "syn_N_nu = 0   # No.of neutrinos generated per season per energy bin per declination bin\n",
    "o_lengths = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for season_i in range(10):\n",
    "\n",
    "\n",
    "def get_syn_nu_dec(season_i):\n",
    "\n",
    "\n",
    "    icdata_k = pd.read_csv(\"./o_data/icecube_10year_ps/events/\" + filenames[season_i], sep=\"\\s+\", comment=\"#\", names=\"MJD[days]\tlog10(E/GeV)\tAngErr[deg]\tRA[deg]\tDec[deg]\tAzimuth[deg]\tZenith[deg]\".split(\"\\t\"), dtype=float)\n",
    "    o_lengths.append(len(icdata_k.index))\n",
    "\n",
    "    uptdata_k = pd.read_csv(\"./o_data/icecube_10year_ps/uptime/\" + filenames[season_i], sep=\"\\s+\", comment=\"#\", names=[\"MJD_start[days]\",\"MJD_stop[days]\"], dtype=float, index_col=False)\n",
    "    \n",
    "    max_stop = max(uptdata_k[\"MJD_stop[days]\"].values)\n",
    "    min_start = min(uptdata_k[\"MJD_start[days]\"].values)\n",
    "    # uptdata_k = []\n",
    "    icsmear_k = []\n",
    "    try:\n",
    "        icsmear_k = pd.read_csv(\"./o_data/icecube_10year_ps/irfs/\" + filenames[season_i].replace('exp', 'smearing'), sep=\"\\s+\", comment=\"#\", names='log10(E_nu/GeV)_min\tlog10(E_nu/GeV)_max\tDec_nu_min[deg]\tDec_nu_max[deg]\tlog10(E/GeV)_min\tlog10(E/GeV)_max\tPSF_min[deg]\tPSF_max[deg]\tAngErr_min[deg]\tAngErr_max[deg]\tFractional_Counts'.split('\\t'), dtype=float)\n",
    "    except:\n",
    "        icsmear_k = pd.read_csv(\"./o_data/icecube_10year_ps/irfs/\" + filenames[4].replace('exp', 'smearing'), sep=\"\\s+\", comment=\"#\", names='log10(E_nu/GeV)_min\tlog10(E_nu/GeV)_max\tDec_nu_min[deg]\tDec_nu_max[deg]\tlog10(E/GeV)_min\tlog10(E/GeV)_max\tPSF_min[deg]\tPSF_max[deg]\tAngErr_min[deg]\tAngErr_max[deg]\tFractional_Counts'.split('\\t'), dtype=float)\n",
    "        # print(temp1)\n",
    "    enus_smear_bin_min = icsmear_k_log_E[ np.digitize(np.log10(enus/1e9), icsmear_k_log_E) - 1]\n",
    "    nu_smear_dec_bin_min = icsmear_k_dec[ np.digitize(syn_nu_dec, icsmear_k_dec) - 1]\n",
    "    \n",
    "    for i in tqdm(range(len(enus))):    \n",
    "        # print(i, enus_smear_bin_min[i])\n",
    "        temp1 = icsmear_k[icsmear_k[\"log10(E_nu/GeV)_min\"] == enus_smear_bin_min[i]]\n",
    "        # print(icsmear_k[\"log10(E_nu/GeV)_min\"].value_counts())\n",
    "        for j in range(len(syn_nu_dec)):\n",
    "        \n",
    "            temp2 = temp1[temp1[\"Dec_nu_min[deg]\"] == nu_smear_dec_bin_min[j]]        \n",
    "            # print(len(temp2))\n",
    "        \n",
    "            try:\n",
    "                psf_avg = (min(temp2[\"PSF_min[deg]\"]) + max(temp2[\"PSF_max[deg]\"])) / 2\n",
    "            except:\n",
    "                print(np.log10(enus_smear_bin_min[i]-9), nu_smear_dec_bin_min[j])\n",
    "                print(temp1)\n",
    "       \n",
    "            angerr_avg = (min(temp2[\"AngErr_min[deg]\"]) + max(temp2[\"AngErr_max[deg]\"])) / 2\n",
    "            theta_tot = np.sqrt(psf_avg**2 + angerr_avg**2)\n",
    "        \n",
    "            # No.of neutrinos to be generated in this season, in this energy bin, in this declination bin\n",
    "            n_nu_temp = t_upt[season_i] * earea[ea_season(season_i)][syn_nudec_bin[j] * 40 + enus_bin_indices[i]] * enus[i] * phio_const * ((enus[i] / 10**14) ** gamma_arr[2])\n",
    "            # print(n_nu_temp, i, j)\n",
    "            n_nu_temp = int(n_nu_temp)\n",
    "            if n_nu_temp > 0:\n",
    "                mjd = np.random.uniform(min_start, max_stop, n_nu_temp)\n",
    "                temp_ra = np.ones(n_nu_temp, dtype=np.float64) * syn_nu_ra[j]\n",
    "                temp_dec = np.ones(n_nu_temp, dtype=np.float64) * syn_nu_dec[j]\n",
    "                \n",
    "                \n",
    "                # print(type(temp_ra), type(temp_dec), type(mjd))\n",
    "                # print(temp_ra.shape, temp_dec.shape, mjd.shape)\n",
    "                syn_nu_az, syn_nu_alt = ra_dec_to_azimuth_zenith(temp_ra, temp_dec, mjd)\n",
    "                \n",
    "                errs = np.random.normal(0, theta_tot/(2**0.5), n_nu_temp)\n",
    "                \n",
    "                syn_nu_az += errs\n",
    "                syn_nu_alt += errs\n",
    "                \n",
    "                for xx in prange(len(syn_nu_alt)):\n",
    "                    if syn_nu_alt[xx] > 90:\n",
    "                        syn_nu_alt[xx] = 180 - syn_nu_alt[xx]\n",
    "                    \n",
    "                    elif syn_nu_alt[xx] < -90:\n",
    "                        syn_nu_alt[xx] = -180 - syn_nu_alt[xx]\n",
    "                    else:\n",
    "                        pass\n",
    "                    \n",
    "                \n",
    "                \n",
    "                syn_ra, syn_dec = [], []\n",
    "                # for taz, tze, tmjd in enumerate(zip(syn_nu_az, syn_nu_ze, mjd)):\n",
    "                #     tra, tdec = altaz_to_radec(taz, tze, tmjd)\n",
    "                #     syn_ra.append(tra)\n",
    "                #     syn_dec.append(tdec)\n",
    "                for temp_i in range(len(syn_nu_az)):\n",
    "                    tra, tdec = altaz_to_radec(syn_nu_az[temp_i], syn_nu_alt[temp_i], mjd[temp_i])\n",
    "                    syn_ra.append(tra)\n",
    "                    syn_dec.append(tdec)\n",
    "                    \n",
    "                syn_ra = np.array(syn_ra)\n",
    "                syn_dec = np.array(syn_dec)\n",
    "                # syn_ra, syn_dec = altaz_to_radec(syn_nu_az, syn_nu_ze, mjd)\n",
    "                \n",
    "                # print(type(syn_nu_ze), type(syn_nu_az))\n",
    "                \n",
    "                temp3 = pd.DataFrame(np.vstack([mjd, -1 * np.ones(len(mjd)), errs, syn_ra, syn_dec, syn_nu_az, syn_nu_alt]).T)\n",
    "                \n",
    "                icdata_k = pd.concat([icdata_k, temp3], ignore_index=True)\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                continue\n",
    "    icdata_k.to_csv(\"./data/icecube_10year_ps/irfs/\" + filenames[season_i], sep=\"\\t\", index=False)\n",
    "    print(\"Done with season\", season_i)\n",
    "    \n",
    "    print(season_i, len(icdata_k.index))\n",
    "    return 0\n",
    "    \n",
    "pool = mul.Pool(12)\n",
    "op_async = pool.map_async(get_syn_nu_dec, tqdm(range(10)))\n",
    "drive = op_async.get()\n",
    "pool.close()\n",
    "pool.join()\n",
    "op_async = []\n",
    "print(\"Generated Synthetic neutrinos and added to original IceCube Data\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
